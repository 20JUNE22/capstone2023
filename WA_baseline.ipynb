{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d21f307f",
   "metadata": {
    "id": "d21f307f"
   },
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "204ae1eb",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 423
    },
    "id": "204ae1eb",
    "outputId": "0e795a13-8fc9-41ee-e382-f1963d38cf2b"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>EastofWA</th>\n",
       "      <th>PCTGQTRS</th>\n",
       "      <th>PovertyRate</th>\n",
       "      <th>MedianFamilyIncome</th>\n",
       "      <th>TractKids</th>\n",
       "      <th>TractSeniors</th>\n",
       "      <th>TractWhite</th>\n",
       "      <th>TractBlack</th>\n",
       "      <th>TractAsian</th>\n",
       "      <th>TractNHOPI</th>\n",
       "      <th>...</th>\n",
       "      <th>log10_TractSeniors</th>\n",
       "      <th>log10_TractWhite</th>\n",
       "      <th>log10_TractBlack</th>\n",
       "      <th>log10_TractAsian</th>\n",
       "      <th>log10_TractNHOPI</th>\n",
       "      <th>log10_TractAIAN</th>\n",
       "      <th>log10_TractOMultir</th>\n",
       "      <th>log10_TractHispanic</th>\n",
       "      <th>log10_TractHUNV</th>\n",
       "      <th>log10_TractSNAP</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>2.41</td>\n",
       "      <td>11.0</td>\n",
       "      <td>67703</td>\n",
       "      <td>534</td>\n",
       "      <td>516</td>\n",
       "      <td>2326</td>\n",
       "      <td>2</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>2.713491</td>\n",
       "      <td>3.348500</td>\n",
       "      <td>0.477121</td>\n",
       "      <td>1.079181</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.397940</td>\n",
       "      <td>1.924279</td>\n",
       "      <td>2.053078</td>\n",
       "      <td>1.944483</td>\n",
       "      <td>2.212188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0.00</td>\n",
       "      <td>18.2</td>\n",
       "      <td>62500</td>\n",
       "      <td>435</td>\n",
       "      <td>305</td>\n",
       "      <td>1524</td>\n",
       "      <td>24</td>\n",
       "      <td>12</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>2.485721</td>\n",
       "      <td>3.155032</td>\n",
       "      <td>1.397940</td>\n",
       "      <td>1.113943</td>\n",
       "      <td>0.477121</td>\n",
       "      <td>1.041393</td>\n",
       "      <td>2.220108</td>\n",
       "      <td>2.307496</td>\n",
       "      <td>1.204120</td>\n",
       "      <td>2.103804</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0.42</td>\n",
       "      <td>29.0</td>\n",
       "      <td>42900</td>\n",
       "      <td>2656</td>\n",
       "      <td>477</td>\n",
       "      <td>3747</td>\n",
       "      <td>48</td>\n",
       "      <td>15</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>2.679428</td>\n",
       "      <td>3.562531</td>\n",
       "      <td>1.690196</td>\n",
       "      <td>1.204120</td>\n",
       "      <td>0.477121</td>\n",
       "      <td>2.217484</td>\n",
       "      <td>3.470998</td>\n",
       "      <td>3.704494</td>\n",
       "      <td>2.096910</td>\n",
       "      <td>2.589950</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>2.59</td>\n",
       "      <td>27.0</td>\n",
       "      <td>55521</td>\n",
       "      <td>1018</td>\n",
       "      <td>330</td>\n",
       "      <td>1764</td>\n",
       "      <td>13</td>\n",
       "      <td>29</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>2.519828</td>\n",
       "      <td>3.222456</td>\n",
       "      <td>1.146128</td>\n",
       "      <td>1.477121</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.707570</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.295347</td>\n",
       "      <td>2.060698</td>\n",
       "      <td>2.264818</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>0.00</td>\n",
       "      <td>28.0</td>\n",
       "      <td>57831</td>\n",
       "      <td>1869</td>\n",
       "      <td>287</td>\n",
       "      <td>2342</td>\n",
       "      <td>22</td>\n",
       "      <td>58</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>2.459392</td>\n",
       "      <td>3.351603</td>\n",
       "      <td>1.361728</td>\n",
       "      <td>1.770852</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.037426</td>\n",
       "      <td>3.346939</td>\n",
       "      <td>3.574031</td>\n",
       "      <td>1.041393</td>\n",
       "      <td>2.469822</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1434</th>\n",
       "      <td>1</td>\n",
       "      <td>1.44</td>\n",
       "      <td>29.4</td>\n",
       "      <td>52059</td>\n",
       "      <td>1441</td>\n",
       "      <td>473</td>\n",
       "      <td>1459</td>\n",
       "      <td>17</td>\n",
       "      <td>42</td>\n",
       "      <td>7</td>\n",
       "      <td>...</td>\n",
       "      <td>2.675778</td>\n",
       "      <td>3.134814</td>\n",
       "      <td>1.255273</td>\n",
       "      <td>1.633468</td>\n",
       "      <td>0.903090</td>\n",
       "      <td>3.177248</td>\n",
       "      <td>3.146748</td>\n",
       "      <td>3.281488</td>\n",
       "      <td>1.806180</td>\n",
       "      <td>2.526339</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1435</th>\n",
       "      <td>1</td>\n",
       "      <td>0.08</td>\n",
       "      <td>23.4</td>\n",
       "      <td>51875</td>\n",
       "      <td>1171</td>\n",
       "      <td>431</td>\n",
       "      <td>1164</td>\n",
       "      <td>12</td>\n",
       "      <td>13</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>2.635484</td>\n",
       "      <td>3.028978</td>\n",
       "      <td>1.113943</td>\n",
       "      <td>1.146128</td>\n",
       "      <td>0.602060</td>\n",
       "      <td>3.285557</td>\n",
       "      <td>2.748963</td>\n",
       "      <td>2.833784</td>\n",
       "      <td>1.799341</td>\n",
       "      <td>2.340444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1436</th>\n",
       "      <td>1</td>\n",
       "      <td>2.76</td>\n",
       "      <td>31.2</td>\n",
       "      <td>36659</td>\n",
       "      <td>2520</td>\n",
       "      <td>440</td>\n",
       "      <td>1106</td>\n",
       "      <td>31</td>\n",
       "      <td>130</td>\n",
       "      <td>7</td>\n",
       "      <td>...</td>\n",
       "      <td>2.644439</td>\n",
       "      <td>3.004751</td>\n",
       "      <td>1.505150</td>\n",
       "      <td>2.117271</td>\n",
       "      <td>0.903090</td>\n",
       "      <td>3.007748</td>\n",
       "      <td>3.651859</td>\n",
       "      <td>3.707655</td>\n",
       "      <td>2.012837</td>\n",
       "      <td>2.794488</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1437</th>\n",
       "      <td>1</td>\n",
       "      <td>1.73</td>\n",
       "      <td>21.2</td>\n",
       "      <td>44451</td>\n",
       "      <td>1711</td>\n",
       "      <td>364</td>\n",
       "      <td>1750</td>\n",
       "      <td>30</td>\n",
       "      <td>23</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>2.562293</td>\n",
       "      <td>3.218798</td>\n",
       "      <td>1.491362</td>\n",
       "      <td>1.380211</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.690196</td>\n",
       "      <td>3.380754</td>\n",
       "      <td>3.555820</td>\n",
       "      <td>1.924279</td>\n",
       "      <td>2.652246</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1438</th>\n",
       "      <td>1</td>\n",
       "      <td>0.00</td>\n",
       "      <td>31.4</td>\n",
       "      <td>45495</td>\n",
       "      <td>1852</td>\n",
       "      <td>277</td>\n",
       "      <td>1400</td>\n",
       "      <td>36</td>\n",
       "      <td>10</td>\n",
       "      <td>7</td>\n",
       "      <td>...</td>\n",
       "      <td>2.444045</td>\n",
       "      <td>3.115611</td>\n",
       "      <td>1.568202</td>\n",
       "      <td>1.041393</td>\n",
       "      <td>0.903090</td>\n",
       "      <td>2.715167</td>\n",
       "      <td>3.457579</td>\n",
       "      <td>3.609914</td>\n",
       "      <td>1.924279</td>\n",
       "      <td>2.720986</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1439 rows × 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      EastofWA  PCTGQTRS  PovertyRate  MedianFamilyIncome  TractKids  \\\n",
       "0            1      2.41         11.0               67703        534   \n",
       "1            1      0.00         18.2               62500        435   \n",
       "2            1      0.42         29.0               42900       2656   \n",
       "3            1      2.59         27.0               55521       1018   \n",
       "4            1      0.00         28.0               57831       1869   \n",
       "...        ...       ...          ...                 ...        ...   \n",
       "1434         1      1.44         29.4               52059       1441   \n",
       "1435         1      0.08         23.4               51875       1171   \n",
       "1436         1      2.76         31.2               36659       2520   \n",
       "1437         1      1.73         21.2               44451       1711   \n",
       "1438         1      0.00         31.4               45495       1852   \n",
       "\n",
       "      TractSeniors  TractWhite  TractBlack  TractAsian  TractNHOPI  ...  \\\n",
       "0              516        2326           2          11           0  ...   \n",
       "1              305        1524          24          12           2  ...   \n",
       "2              477        3747          48          15           2  ...   \n",
       "3              330        1764          13          29           0  ...   \n",
       "4              287        2342          22          58           0  ...   \n",
       "...            ...         ...         ...         ...         ...  ...   \n",
       "1434           473        1459          17          42           7  ...   \n",
       "1435           431        1164          12          13           3  ...   \n",
       "1436           440        1106          31         130           7  ...   \n",
       "1437           364        1750          30          23           0  ...   \n",
       "1438           277        1400          36          10           7  ...   \n",
       "\n",
       "      log10_TractSeniors  log10_TractWhite  log10_TractBlack  \\\n",
       "0               2.713491          3.348500          0.477121   \n",
       "1               2.485721          3.155032          1.397940   \n",
       "2               2.679428          3.562531          1.690196   \n",
       "3               2.519828          3.222456          1.146128   \n",
       "4               2.459392          3.351603          1.361728   \n",
       "...                  ...               ...               ...   \n",
       "1434            2.675778          3.134814          1.255273   \n",
       "1435            2.635484          3.028978          1.113943   \n",
       "1436            2.644439          3.004751          1.505150   \n",
       "1437            2.562293          3.218798          1.491362   \n",
       "1438            2.444045          3.115611          1.568202   \n",
       "\n",
       "      log10_TractAsian  log10_TractNHOPI  log10_TractAIAN  log10_TractOMultir  \\\n",
       "0             1.079181          0.000000         1.397940            1.924279   \n",
       "1             1.113943          0.477121         1.041393            2.220108   \n",
       "2             1.204120          0.477121         2.217484            3.470998   \n",
       "3             1.477121          0.000000         1.707570            3.000000   \n",
       "4             1.770852          0.000000         2.037426            3.346939   \n",
       "...                ...               ...              ...                 ...   \n",
       "1434          1.633468          0.903090         3.177248            3.146748   \n",
       "1435          1.146128          0.602060         3.285557            2.748963   \n",
       "1436          2.117271          0.903090         3.007748            3.651859   \n",
       "1437          1.380211          0.000000         2.690196            3.380754   \n",
       "1438          1.041393          0.903090         2.715167            3.457579   \n",
       "\n",
       "      log10_TractHispanic  log10_TractHUNV  log10_TractSNAP  \n",
       "0                2.053078         1.944483         2.212188  \n",
       "1                2.307496         1.204120         2.103804  \n",
       "2                3.704494         2.096910         2.589950  \n",
       "3                3.295347         2.060698         2.264818  \n",
       "4                3.574031         1.041393         2.469822  \n",
       "...                   ...              ...              ...  \n",
       "1434             3.281488         1.806180         2.526339  \n",
       "1435             2.833784         1.799341         2.340444  \n",
       "1436             3.707655         2.012837         2.794488  \n",
       "1437             3.555820         1.924279         2.652246  \n",
       "1438             3.609914         1.924279         2.720986  \n",
       "\n",
       "[1439 rows x 30 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fd = pd.read_excel(\"https://github.com/20JUNE22/capstone2023/raw/main/clean_wa.xlsx\")\n",
    " \n",
    "fd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6011af23",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "6011af23",
    "outputId": "a509fc2c-b75a-4b7d-e15e-6ffe645d7355"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>EastofWA</th>\n",
       "      <th>FoodDesert</th>\n",
       "      <th>log10_PCTGQTRS</th>\n",
       "      <th>log10_PovertyRate</th>\n",
       "      <th>log10_MedianFamilyIncome</th>\n",
       "      <th>log10_TractKids</th>\n",
       "      <th>log10_TractSeniors</th>\n",
       "      <th>log10_TractWhite</th>\n",
       "      <th>log10_TractBlack</th>\n",
       "      <th>log10_TractAsian</th>\n",
       "      <th>log10_TractNHOPI</th>\n",
       "      <th>log10_TractAIAN</th>\n",
       "      <th>log10_TractOMultir</th>\n",
       "      <th>log10_TractHispanic</th>\n",
       "      <th>log10_TractHUNV</th>\n",
       "      <th>log10_TractSNAP</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.532754</td>\n",
       "      <td>1.075547</td>\n",
       "      <td>4.655551</td>\n",
       "      <td>2.719331</td>\n",
       "      <td>2.713491</td>\n",
       "      <td>3.348500</td>\n",
       "      <td>0.477121</td>\n",
       "      <td>1.079181</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.397940</td>\n",
       "      <td>1.924279</td>\n",
       "      <td>2.053078</td>\n",
       "      <td>1.944483</td>\n",
       "      <td>2.212188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.281033</td>\n",
       "      <td>4.602494</td>\n",
       "      <td>2.628389</td>\n",
       "      <td>2.485721</td>\n",
       "      <td>3.155032</td>\n",
       "      <td>1.397940</td>\n",
       "      <td>1.113943</td>\n",
       "      <td>0.477121</td>\n",
       "      <td>1.041393</td>\n",
       "      <td>2.220108</td>\n",
       "      <td>2.307496</td>\n",
       "      <td>1.204120</td>\n",
       "      <td>2.103804</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.152288</td>\n",
       "      <td>1.475671</td>\n",
       "      <td>4.310481</td>\n",
       "      <td>3.422590</td>\n",
       "      <td>2.679428</td>\n",
       "      <td>3.562531</td>\n",
       "      <td>1.690196</td>\n",
       "      <td>1.204120</td>\n",
       "      <td>0.477121</td>\n",
       "      <td>2.217484</td>\n",
       "      <td>3.470998</td>\n",
       "      <td>3.704494</td>\n",
       "      <td>2.096910</td>\n",
       "      <td>2.589950</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.555094</td>\n",
       "      <td>1.445604</td>\n",
       "      <td>4.519316</td>\n",
       "      <td>3.003461</td>\n",
       "      <td>2.519828</td>\n",
       "      <td>3.222456</td>\n",
       "      <td>1.146128</td>\n",
       "      <td>1.477121</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.707570</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.295347</td>\n",
       "      <td>2.060698</td>\n",
       "      <td>2.264818</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.460898</td>\n",
       "      <td>4.548647</td>\n",
       "      <td>3.269279</td>\n",
       "      <td>2.459392</td>\n",
       "      <td>3.351603</td>\n",
       "      <td>1.361728</td>\n",
       "      <td>1.770852</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.037426</td>\n",
       "      <td>3.346939</td>\n",
       "      <td>3.574031</td>\n",
       "      <td>1.041393</td>\n",
       "      <td>2.469822</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   EastofWA  FoodDesert  log10_PCTGQTRS  log10_PovertyRate  \\\n",
       "0         1           0        0.532754           1.075547   \n",
       "1         1           1        0.000000           1.281033   \n",
       "2         1           1        0.152288           1.475671   \n",
       "3         1           0        0.555094           1.445604   \n",
       "4         1           0        0.000000           1.460898   \n",
       "\n",
       "   log10_MedianFamilyIncome  log10_TractKids  log10_TractSeniors  \\\n",
       "0                  4.655551         2.719331            2.713491   \n",
       "1                  4.602494         2.628389            2.485721   \n",
       "2                  4.310481         3.422590            2.679428   \n",
       "3                  4.519316         3.003461            2.519828   \n",
       "4                  4.548647         3.269279            2.459392   \n",
       "\n",
       "   log10_TractWhite  log10_TractBlack  log10_TractAsian  log10_TractNHOPI  \\\n",
       "0          3.348500          0.477121          1.079181          0.000000   \n",
       "1          3.155032          1.397940          1.113943          0.477121   \n",
       "2          3.562531          1.690196          1.204120          0.477121   \n",
       "3          3.222456          1.146128          1.477121          0.000000   \n",
       "4          3.351603          1.361728          1.770852          0.000000   \n",
       "\n",
       "   log10_TractAIAN  log10_TractOMultir  log10_TractHispanic  log10_TractHUNV  \\\n",
       "0         1.397940            1.924279             2.053078         1.944483   \n",
       "1         1.041393            2.220108             2.307496         1.204120   \n",
       "2         2.217484            3.470998             3.704494         2.096910   \n",
       "3         1.707570            3.000000             3.295347         2.060698   \n",
       "4         2.037426            3.346939             3.574031         1.041393   \n",
       "\n",
       "   log10_TractSNAP  \n",
       "0         2.212188  \n",
       "1         2.103804  \n",
       "2         2.589950  \n",
       "3         2.264818  \n",
       "4         2.469822  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# drop the irrelevant inputs\n",
    "wa = fd.drop(fd.columns[1:15], axis=1)\n",
    "\n",
    "\n",
    "wa.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ad875ab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# divided wa data into the independent and dependent variables\n",
    "X = wa.drop(['FoodDesert'], axis = 1) # all independent\n",
    "y = wa['FoodDesert']  # target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ce1b16fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>EastofWA</th>\n",
       "      <th>log10_PCTGQTRS</th>\n",
       "      <th>log10_PovertyRate</th>\n",
       "      <th>log10_MedianFamilyIncome</th>\n",
       "      <th>log10_TractKids</th>\n",
       "      <th>log10_TractSeniors</th>\n",
       "      <th>log10_TractWhite</th>\n",
       "      <th>log10_TractBlack</th>\n",
       "      <th>log10_TractAsian</th>\n",
       "      <th>log10_TractNHOPI</th>\n",
       "      <th>log10_TractAIAN</th>\n",
       "      <th>log10_TractOMultir</th>\n",
       "      <th>log10_TractHispanic</th>\n",
       "      <th>log10_TractHUNV</th>\n",
       "      <th>log10_TractSNAP</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0.532754</td>\n",
       "      <td>1.075547</td>\n",
       "      <td>4.655551</td>\n",
       "      <td>2.719331</td>\n",
       "      <td>2.713491</td>\n",
       "      <td>3.348500</td>\n",
       "      <td>0.477121</td>\n",
       "      <td>1.079181</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.397940</td>\n",
       "      <td>1.924279</td>\n",
       "      <td>2.053078</td>\n",
       "      <td>1.944483</td>\n",
       "      <td>2.212188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.281033</td>\n",
       "      <td>4.602494</td>\n",
       "      <td>2.628389</td>\n",
       "      <td>2.485721</td>\n",
       "      <td>3.155032</td>\n",
       "      <td>1.397940</td>\n",
       "      <td>1.113943</td>\n",
       "      <td>0.477121</td>\n",
       "      <td>1.041393</td>\n",
       "      <td>2.220108</td>\n",
       "      <td>2.307496</td>\n",
       "      <td>1.204120</td>\n",
       "      <td>2.103804</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0.152288</td>\n",
       "      <td>1.475671</td>\n",
       "      <td>4.310481</td>\n",
       "      <td>3.422590</td>\n",
       "      <td>2.679428</td>\n",
       "      <td>3.562531</td>\n",
       "      <td>1.690196</td>\n",
       "      <td>1.204120</td>\n",
       "      <td>0.477121</td>\n",
       "      <td>2.217484</td>\n",
       "      <td>3.470998</td>\n",
       "      <td>3.704494</td>\n",
       "      <td>2.096910</td>\n",
       "      <td>2.589950</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>0.555094</td>\n",
       "      <td>1.445604</td>\n",
       "      <td>4.519316</td>\n",
       "      <td>3.003461</td>\n",
       "      <td>2.519828</td>\n",
       "      <td>3.222456</td>\n",
       "      <td>1.146128</td>\n",
       "      <td>1.477121</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.707570</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.295347</td>\n",
       "      <td>2.060698</td>\n",
       "      <td>2.264818</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.460898</td>\n",
       "      <td>4.548647</td>\n",
       "      <td>3.269279</td>\n",
       "      <td>2.459392</td>\n",
       "      <td>3.351603</td>\n",
       "      <td>1.361728</td>\n",
       "      <td>1.770852</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.037426</td>\n",
       "      <td>3.346939</td>\n",
       "      <td>3.574031</td>\n",
       "      <td>1.041393</td>\n",
       "      <td>2.469822</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   EastofWA  log10_PCTGQTRS  log10_PovertyRate  log10_MedianFamilyIncome  \\\n",
       "0         1        0.532754           1.075547                  4.655551   \n",
       "1         1        0.000000           1.281033                  4.602494   \n",
       "2         1        0.152288           1.475671                  4.310481   \n",
       "3         1        0.555094           1.445604                  4.519316   \n",
       "4         1        0.000000           1.460898                  4.548647   \n",
       "\n",
       "   log10_TractKids  log10_TractSeniors  log10_TractWhite  log10_TractBlack  \\\n",
       "0         2.719331            2.713491          3.348500          0.477121   \n",
       "1         2.628389            2.485721          3.155032          1.397940   \n",
       "2         3.422590            2.679428          3.562531          1.690196   \n",
       "3         3.003461            2.519828          3.222456          1.146128   \n",
       "4         3.269279            2.459392          3.351603          1.361728   \n",
       "\n",
       "   log10_TractAsian  log10_TractNHOPI  log10_TractAIAN  log10_TractOMultir  \\\n",
       "0          1.079181          0.000000         1.397940            1.924279   \n",
       "1          1.113943          0.477121         1.041393            2.220108   \n",
       "2          1.204120          0.477121         2.217484            3.470998   \n",
       "3          1.477121          0.000000         1.707570            3.000000   \n",
       "4          1.770852          0.000000         2.037426            3.346939   \n",
       "\n",
       "   log10_TractHispanic  log10_TractHUNV  log10_TractSNAP  \n",
       "0             2.053078         1.944483         2.212188  \n",
       "1             2.307496         1.204120         2.103804  \n",
       "2             3.704494         2.096910         2.589950  \n",
       "3             3.295347         2.060698         2.264818  \n",
       "4             3.574031         1.041393         2.469822  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "436b427b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       0\n",
       "1       1\n",
       "2       1\n",
       "3       0\n",
       "4       0\n",
       "       ..\n",
       "1434    0\n",
       "1435    1\n",
       "1436    0\n",
       "1437    0\n",
       "1438    0\n",
       "Name: FoodDesert, Length: 1439, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a1e917a3",
   "metadata": {
    "id": "a1e917a3",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Split data into independent variables (X) and dependent variable (y)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8413ccae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Results:\n",
      "Accuracy: 0.649\n",
      "Precision: 0.557\n",
      "Recall: 0.482\n",
      "F1 Score: 0.517\n",
      "\n",
      "Decision Tree Results:\n",
      "Accuracy: 0.608\n",
      "Precision: 0.496\n",
      "Recall: 0.580\n",
      "F1 Score: 0.535\n",
      "\n",
      "Random Forest Results:\n",
      "Accuracy: 0.698\n",
      "Precision: 0.624\n",
      "Recall: 0.562\n",
      "F1 Score: 0.592\n",
      "\n",
      "XGBoost Results:\n",
      "Accuracy: 0.663\n",
      "Precision: 0.565\n",
      "Recall: 0.580\n",
      "F1 Score: 0.573\n",
      "\n",
      "LightGBM Results:\n",
      "Accuracy: 0.684\n",
      "Precision: 0.600\n",
      "Recall: 0.562\n",
      "F1 Score: 0.581\n",
      "\n",
      "SVM Results:\n",
      "Accuracy: 0.670\n",
      "Precision: 0.616\n",
      "Recall: 0.402\n",
      "F1 Score: 0.486\n",
      "\n",
      "KNN Results:\n",
      "Accuracy: 0.639\n",
      "Precision: 0.535\n",
      "Recall: 0.545\n",
      "F1 Score: 0.540\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# Load your data and split into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define the models you want to compare\n",
    "models = [\n",
    "    ('Logistic Regression', LogisticRegression(max_iter=1000)),\n",
    "    ('Decision Tree', DecisionTreeClassifier()),\n",
    "    ('Random Forest', RandomForestClassifier()),\n",
    "    ('XGBoost', XGBClassifier()),\n",
    "    ('LightGBM', LGBMClassifier()),\n",
    "    ('SVM', SVC()),\n",
    "    ('KNN', KNeighborsClassifier())\n",
    "]\n",
    "\n",
    "# Train and evaluate each model\n",
    "for name, model in models:\n",
    "    # Fit the model to the training data\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Make predictions on the testing data\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    # Evaluate the model's performance using common metrics\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred)\n",
    "    recall = recall_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    \n",
    "    # Print the model's performance metrics\n",
    "    print(f\"{name} Results:\")\n",
    "    print(f\"Accuracy: {accuracy:.3f}\")\n",
    "    print(f\"Precision: {precision:.3f}\")\n",
    "    print(f\"Recall: {recall:.3f}\")\n",
    "    print(f\"F1 Score: {f1:.3f}\")\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbdbd4d0",
   "metadata": {},
   "source": [
    "The classification report shows that the model has an overall accuracy of 0.65, which means that the model correctly predicted the class of 65% of the instances in the test set. The precision for class 0 is 0.70, which means that when the model predicts class 0, it is correct 70% of the time. The recall for class 0 is 0.76, which means that the model correctly identifies 76% of the instances that belong to class 0. The precision for class 1 is 0.56, which means that when the model predicts class 1, it is correct 56% of the time. The recall for class 1 is 0.48, which means that the model correctly identifies 48% of the instances that belong to class 1.\n",
    "\n",
    "The F1-score is a harmonic mean of precision and recall, and it gives an overall measure of the model's performance. The F1-score for class 0 is 0.72, and the F1-score for class 1 is 0.52.\n",
    "\n",
    "The macro-avg F1-score is 0.62, which is the average of the F1-scores for both classes. The weighted-avg F1-score is 0.64, which is the weighted average of the F1-scores for both classes, where the weight is proportional to the number of instances in each class.\n",
    "\n",
    "The ROC-AUC score is 0.62, which means that the model's ability to distinguish between the two classes is slightly better than random guessing. A perfect model has an ROC-AUC score of 1.0, while a random model has an ROC-AUC score of 0.5."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ed3e222",
   "metadata": {},
   "source": [
    "The classification report and ROC-AUC score indicate that the model's performance is fair, but not great. The accuracy score of 0.65 indicates that the model correctly classified 65% of the samples, which is better than random guessing but still has room for improvement. The precision and recall scores are also relatively low, indicating that the model is not able to correctly identify all positive samples and has a higher rate of false positives.\n",
    "\n",
    "In summary, while the model is performing better than random guessing, there is still room for improvement, and it may be beneficial to explore other modeling approaches or data preprocessing techniques to try to improve its performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d2be6296",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline model accuracy: 0.6493055555555556\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.70      0.76      0.72       176\n",
      "           1       0.56      0.48      0.52       112\n",
      "\n",
      "    accuracy                           0.65       288\n",
      "   macro avg       0.63      0.62      0.62       288\n",
      "weighted avg       0.64      0.65      0.64       288\n",
      "\n",
      "ROC-AUC Score: 0.6189123376623376\n",
      "Confusion Matrix:\n",
      " [[133  43]\n",
      " [ 58  54]]\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'plot_roc_curve' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_34836/895592713.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     33\u001b[0m \u001b[1;31m# Plot the ROC curve\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 34\u001b[1;33m \u001b[0mplot_roc_curve\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mBaseline_model\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# use the input data instead of the predicted labels\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     35\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'plot_roc_curve' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import accuracy_score, classification_report, roc_auc_score, confusion_matrix\n",
    "\n",
    "\n",
    "\n",
    "# Fit a logistic regression model\n",
    "Baseline_model = LogisticRegression(max_iter=10000)\n",
    "Baseline_model.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred_proba = Baseline_model.predict_proba(X_test)[:, 1] # predict probabilities for the positive class\n",
    "\n",
    "# Calculate the accuracy of the model\n",
    "y_pred = Baseline_model.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Baseline model accuracy:\", accuracy)\n",
    "\n",
    "# Calculate the classification report\n",
    "report = classification_report(y_test, y_pred)\n",
    "print(\"Classification Report:\")\n",
    "print(report)\n",
    "\n",
    "# Calculate the ROC-AUC score\n",
    "roc_auc = roc_auc_score(y_test, y_pred)\n",
    "print(\"ROC-AUC Score:\", roc_auc)\n",
    "\n",
    "# Calculate the confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(\"Confusion Matrix:\\n\", cm)\n",
    "\n",
    "# Plot the ROC curve\n",
    "plot_roc_curve(Baseline_model, X_test, y_test) # use the input data instead of the predicted labels\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4ffe2f37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.pipeline import Pipeline\n",
    "# from sklearn.preprocessing import StandardScaler\n",
    "# from sklearn.feature_selection import SelectKBest, f_classif\n",
    "# from sklearn.linear_model import LogisticRegression\n",
    "# from sklearn.metrics import accuracy_score\n",
    "\n",
    "# # Define the pipeline\n",
    "# pipeline = Pipeline([\n",
    "#   ('scaler', StandardScaler()), # Scale the features\n",
    "#   ('selector', SelectKBest(f_classif, k=10)), # Select the top 10 features\n",
    "#   ('model', LogisticRegression()) # Train the logistic regression model\n",
    "# ])\n",
    "\n",
    "# # Fit the pipeline on the training data\n",
    "# pipeline.fit(train_data.drop('FoodDesert', axis=1), train_data['FoodDesert'])\n",
    "\n",
    "# # Make predictions on the test data\n",
    "# predictions = pipeline.predict(test_data.drop('FoodDesert', axis=1))\n",
    "\n",
    "# # Evaluate the performance of the model\n",
    "# accuracy = accuracy_score(test_data['FoodDesert'], predictions)\n",
    "# print(\"Accuracy:\", accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "556b81a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline model2 accuracy: 0.7083333333333334\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.73      0.82      0.77       176\n",
      "           1       0.65      0.54      0.59       112\n",
      "\n",
      "    accuracy                           0.71       288\n",
      "   macro avg       0.69      0.68      0.68       288\n",
      "weighted avg       0.70      0.71      0.70       288\n",
      "\n",
      "ROC-AUC Score: 0.6769480519480519\n",
      "Confusion Matrix:\n",
      " [[144  32]\n",
      " [ 52  60]]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report, roc_auc_score, confusion_matrix, plot_roc_curve\n",
    "\n",
    "\n",
    "# Create a baseline Random Forest model\n",
    "Baseline_model2 = RandomForestClassifier(n_estimators=100, random_state=123)\n",
    "\n",
    "# Fit the model on the training data\n",
    "Baseline_model2.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred = Baseline_model2.predict(X_test)\n",
    "\n",
    "# Calculate the accuracy of the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Baseline model2 accuracy:\", accuracy)\n",
    "\n",
    "# Calculate the classification report\n",
    "report = classification_report(y_test, y_pred)\n",
    "print(\"Classification Report:\")\n",
    "print(report)\n",
    "\n",
    "# Calculate the ROC-AUC score\n",
    "roc_auc = roc_auc_score(y_test, y_pred)\n",
    "print(\"ROC-AUC Score:\", roc_auc)\n",
    "\n",
    "# Calculate the confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(\"Confusion Matrix:\\n\", cm)\n",
    "\n",
    "# Plot the ROC curve\n",
    "plot_roc_curve(Baseline_model2, X_test, y_test)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8de3fe2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 491, number of negative: 660\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000485 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 3290\n",
      "[LightGBM] [Info] Number of data points in the train set: 1151, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.426586 -> initscore=-0.295796\n",
      "[LightGBM] [Info] Start training from score -0.295796\n",
      "Baseline model accuracy: 0.6701388888888888\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.73      0.74      0.73       176\n",
      "           1       0.58      0.56      0.57       112\n",
      "\n",
      "    accuracy                           0.67       288\n",
      "   macro avg       0.65      0.65      0.65       288\n",
      "weighted avg       0.67      0.67      0.67       288\n",
      "\n",
      "ROC-AUC Score: 0.6505681818181818\n",
      "Confusion Matrix:\n",
      " [[130  46]\n",
      " [ 49  63]]\n"
     ]
    }
   ],
   "source": [
    "import lightgbm as lgb\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "# Create a LightGBM dataset\n",
    "train_data = lgb.Dataset(X_train, label=y_train)\n",
    "\n",
    "# Set the hyperparameters\n",
    "params = {\n",
    "    'objective': 'binary',\n",
    "    'metric': 'binary_logloss',\n",
    "    'boosting_type': 'gbdt',\n",
    "    'num_leaves': 31,\n",
    "    'learning_rate': 0.05,\n",
    "    'max_depth': -1,\n",
    "    'min_child_samples': 20,\n",
    "    'subsample': 0.8,\n",
    "    'colsample_bytree': 0.8,\n",
    "    'n_jobs': -1,\n",
    "    'random_state': 42\n",
    "}\n",
    "\n",
    "# Train the model\n",
    "num_round = 100\n",
    "Baseline_model = lgb.train(params, train_data, num_round)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = Baseline_model.predict(X_test)\n",
    "y_pred = [1 if pred > 0.5 else 0 for pred in y_pred]\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Baseline model accuracy:\", accuracy)\n",
    "\n",
    "report = classification_report(y_test, y_pred)\n",
    "print(\"Classification Report:\")\n",
    "print(report)\n",
    "\n",
    "roc_auc = roc_auc_score(y_test, y_pred)\n",
    "print(\"ROC-AUC Score:\", roc_auc)\n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(\"Confusion Matrix:\\n\", cm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c0ee19eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000305 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "Baseline model accuracy: 0.6909722222222222\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.75      0.74      0.74       176\n",
      "           1       0.60      0.62      0.61       112\n",
      "\n",
      "    accuracy                           0.69       288\n",
      "   macro avg       0.68      0.68      0.68       288\n",
      "weighted avg       0.69      0.69      0.69       288\n",
      "\n",
      "ROC-AUC Score: 0.7326501623376622\n",
      "Confusion Matrix:\n",
      " [[130  46]\n",
      " [ 43  69]]\n"
     ]
    }
   ],
   "source": [
    "import lightgbm as lgb\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, roc_auc_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create a LightGBM dataset for training\n",
    "train_data = lgb.Dataset(X_train, label=y_train)\n",
    "\n",
    "# Set parameters for the model\n",
    "params = {\n",
    "    'objective': 'binary',\n",
    "    'metric': 'binary_logloss',\n",
    "    'num_leaves': 31,\n",
    "    'learning_rate': 0.05,\n",
    "    'feature_fraction': 0.9,\n",
    "    'bagging_fraction': 0.8,\n",
    "    'bagging_freq': 5,\n",
    "    'verbose': 0\n",
    "}\n",
    "\n",
    "# Train the model\n",
    "Baseline_model = lgb.train(params, train_data, num_boost_round=100)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = Baseline_model.predict(X_test)\n",
    "\n",
    "# Convert probabilities to binary predictions\n",
    "y_pred_binary = [1 if i > 0.5 else 0 for i in y_pred]\n",
    "\n",
    "# Calculate the accuracy of the model\n",
    "accuracy = accuracy_score(y_test, y_pred_binary)\n",
    "print(\"Baseline model accuracy:\", accuracy)\n",
    "\n",
    "# Calculate the classification report\n",
    "report = classification_report(y_test, y_pred_binary)\n",
    "print(\"Classification Report:\")\n",
    "print(report)\n",
    "\n",
    "# Calculate the ROC-AUC score\n",
    "roc_auc = roc_auc_score(y_test, y_pred)\n",
    "print(\"ROC-AUC Score:\", roc_auc)\n",
    "\n",
    "# Calculate the confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred_binary)\n",
    "print(\"Confusion Matrix:\\n\", cm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e5f4564b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6423611111111112\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.70      0.74      0.72       176\n",
      "           1       0.54      0.49      0.52       112\n",
      "\n",
      "    accuracy                           0.64       288\n",
      "   macro avg       0.62      0.61      0.62       288\n",
      "weighted avg       0.64      0.64      0.64       288\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Create the Discriminant Analysis model\n",
    "model = LinearDiscriminantAnalysis()\n",
    "\n",
    "# Fit the model on the training data\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test data\n",
    "predictions = model.predict(X_test)\n",
    "\n",
    "# Evaluate the performance of the model\n",
    "accuracy = accuracy_score(y_test, predictions)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "\n",
    "report = classification_report(y_test, predictions)\n",
    "print(\"Classification Report:\\n\", report)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bb2424d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6597222222222222\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "from sklearn.linear_model import RidgeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n",
    "\n",
    "# Create a pipeline for feature selection and regularization\n",
    "pipe = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('feature_selection', SelectKBest(f_classif, k=5)),\n",
    "    ('classifier', RidgeClassifier(alpha=0.1))\n",
    "])\n",
    "\n",
    "# Fit the pipeline on the training data\n",
    "pipe.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test data\n",
    "predictions = pipe.predict(X_test)\n",
    "\n",
    "# Evaluate the performance of the model\n",
    "accuracy = accuracy_score(y_test, predictions)\n",
    "print(\"Accuracy:\", accuracy)\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
